Тестируем BERT — нейронную сеть от Google, показавшую с большим отрывом state-of-the-art результаты на целом ряде задач (2019г.). С помощью BERT можно создавать программы с ИИ для обработки естественного языка: отвечать на вопросы, заданные в произвольной форме, создавать чат-ботов, автоматические переводчики, анализировать текст.

Идея в основе BERT лежит очень простая: подавать на вход нейросети фразы, в которых 15% слов заменили на [MASK], и обучить нейронную сеть предсказывать эти закрытые маской слова.


Например, если подаем на вход нейросети фразу "Я пришел в [MASK] и купил [MASK]", она должна на выходе показать слова "магазин" и "молоко". Это упрощенный пример с официальной страницы BERT, на более длинных предложениях разброс возможных вариантов становится меньше, а ответ нейросети однозначнее.


Для того, чтобы нейросеть научилась понимать соотношения между разными предложениями, нужно дополнительно обучить ее предсказывать, является ли вторая фраза логичным продолжением первой, или это какая-то случайная фраза, не имеющая никакого отношения к первой.


Так, для двух предложений: "Я пошел в магазин." и "И купил там молоко.", нейросеть должна ответить, что это логично. А если вторая фраза будет "Карась небо Плутон", то должна ответить, что это предложение никак не связано с первым. 
